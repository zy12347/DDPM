{"cells":[{"cell_type":"code","execution_count":null,"id":"40564056","metadata":{"id":"40564056"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"id":"dd9b9948","metadata":{"id":"dd9b9948"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, channels, size):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.size = size\n","        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n","        self.ln = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n","        x_ln = self.ln(x)\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b68a02e2","metadata":{"id":"b68a02e2"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n","        super().__init__()\n","        self.residual = residual\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, mid_channels),\n","            nn.GELU(),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, out_channels),\n","        )\n","\n","    def forward(self, x):\n","        if self.residual:\n","            return F.gelu(x + self.double_conv(x))\n","        else:\n","            return self.double_conv(x)"]},{"cell_type":"code","execution_count":null,"id":"88b050e2","metadata":{"id":"88b050e2"},"outputs":[],"source":["class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, t):\n","        x = self.maxpool_conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb"]},{"cell_type":"code","execution_count":null,"id":"c9f8d4f9","metadata":{"id":"c9f8d4f9"},"outputs":[],"source":["class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.conv = nn.Sequential(\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels, in_channels // 2),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, skip_x, t):\n","        x = self.up(x)\n","        x = torch.cat([skip_x, x], dim=1)\n","        x = self.conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb"]},{"cell_type":"code","source":["class EMA:\n","    def __init__(self, beta):\n","        super().__init__()\n","        self.beta = beta\n","        self.step = 0\n","\n","    def update_model_average(self, ma_model, current_model):\n","        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","            old_weight, up_weight = ma_params.data, current_params.data\n","            ma_params.data = self.update_average(old_weight, up_weight)\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","        return old * self.beta + (1 - self.beta) * new\n","\n","    def step_ema(self, ema_model, model, step_start_ema=2000):\n","        if self.step < step_start_ema:\n","            self.reset_parameters(ema_model, model)\n","            self.step += 1\n","            return\n","        self.update_model_average(ema_model, model)\n","        self.step += 1\n","\n","    def reset_parameters(self, ema_model, model):\n","        ema_model.load_state_dict(model.state_dict())"],"metadata":{"id":"f_Bww5KTcZnb"},"id":"f_Bww5KTcZnb","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"919890fb","metadata":{"id":"919890fb"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, img_size=32, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n","        super().__init__()\n","        if img_size==32:\n","            self.device = device\n","            self.time_dim = time_dim\n","            self.inc = DoubleConv(c_in, 32)\n","            self.down1 = Down(32, 64)\n","            self.sa1 = SelfAttention(64, img_size//2)\n","            self.down2 = Down(64, 128)\n","            self.sa2 = SelfAttention(128, img_size//4)\n","            self.down3 = Down(128, 128)\n","            self.sa3 = SelfAttention(128, img_size//8)\n","\n","            self.bot1 = DoubleConv(128, 256)\n","            self.bot2 = DoubleConv(256, 256)\n","            self.bot3 = DoubleConv(256, 128)\n","\n","            self.up1 = Up(256, 64)\n","            self.sa4 = SelfAttention(64, img_size//4)\n","            self.up2 = Up(128, 32)\n","            self.sa5 = SelfAttention(32, img_size//2)\n","            self.up3 = Up(64, 32)\n","            self.sa6 = SelfAttention(32, img_size)\n","            self.outc = nn.Conv2d(32, c_out, kernel_size=1)\n","        elif img_size==64:   # 5 3 64 64\n","            self.device = device\n","            self.time_dim = time_dim\n","            self.inc = DoubleConv(c_in, 64)  #5 64 64 64\n","            self.down1 = Down(64, 128)   # 5 128 32 32\n","            self.sa1 = SelfAttention(128, img_size//2) # _20_ 128  16  16\n","            self.down2 = Down(128, 256)\n","            self.sa2 = SelfAttention(256, img_size//4)\n","            self.down3 = Down(256, 256)\n","            self.sa3 = SelfAttention(256, img_size//8)\n","\n","            self.bot1 = DoubleConv(256, 512)\n","            self.bot2 = DoubleConv(512, 512)\n","            self.bot3 = DoubleConv(512, 256)\n","\n","            self.up1 = Up(512, 128)\n","            self.sa4 = SelfAttention(128, img_size//4)\n","            self.up2 = Up(256, 64)\n","            self.sa5 = SelfAttention(64, img_size//2)\n","            self.up3 = Up(128, 64)\n","            self.sa6 = SelfAttention(64, img_size)\n","            self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n","\n","    def pos_encoding(self, t, channels, device='cuda'):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq).to(device)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq).to(device)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1).to(device)\n","        return pos_enc\n","\n","    def forward(self, x, t):\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.time_dim)\n","\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","        x4 = self.down3(x3, t)\n","        x4 = self.sa3(x4)\n","        #print(x4.shape) torch.Size([5, 256, 8, 8])\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","        #print(x4.shape) torch.Size([5, 256, 8, 8])\n","        x = self.up1(x4, x3, t)\n","        x = self.sa4(x)\n","        x = self.up2(x, x2, t)\n","        x = self.sa5(x)\n","        x = self.up3(x, x1, t)\n","        x = self.sa6(x)\n","        output = self.outc(x)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"id":"e729fa78","metadata":{"id":"e729fa78"},"outputs":[],"source":["class UNet_conditional(nn.Module):\n","    def __init__(self, img_size=32, c_in=3, c_out=3, time_dim=256, num_classes=None, device=\"cuda\"):\n","        super().__init__()\n","        if img_size==32:\n","            self.device = device\n","            self.time_dim = time_dim\n","            self.inc = DoubleConv(c_in, 32)\n","            self.down1 = Down(32, 64)\n","            self.sa1 = SelfAttention(64, img_size//2)\n","            self.down2 = Down(64, 128)\n","            self.sa2 = SelfAttention(128, img_size//4)\n","            self.down3 = Down(128, 128)\n","            self.sa3 = SelfAttention(128, img_size//8)\n","\n","            self.bot1 = DoubleConv(128, 256)\n","            self.bot2 = DoubleConv(256, 256)\n","            self.bot3 = DoubleConv(256, 128)\n","\n","            self.up1 = Up(256, 64)\n","            self.sa4 = SelfAttention(64, img_size//4)\n","            self.up2 = Up(128, 32)\n","            self.sa5 = SelfAttention(32, img_size//2)\n","            self.up3 = Up(64, 32)\n","            self.sa6 = SelfAttention(32, img_size)\n","            self.outc = nn.Conv2d(32, c_out, kernel_size=1)\n","        elif img_size==64:   # 5 3 64 64\n","            self.device = device\n","            self.time_dim = time_dim\n","            self.inc = DoubleConv(c_in, 64)  #5 64 64 64\n","            self.down1 = Down(64, 128)   # 5 128 32 32\n","            self.sa1 = SelfAttention(128, img_size//2) # _20_ 128  16  16\n","            self.down2 = Down(128, 256)\n","            self.sa2 = SelfAttention(256, img_size//4)\n","            self.down3 = Down(256, 256)\n","            self.sa3 = SelfAttention(256, img_size//8)\n","\n","            self.bot1 = DoubleConv(256, 512)\n","            self.bot2 = DoubleConv(512, 512)\n","            self.bot3 = DoubleConv(512, 256)\n","\n","            self.up1 = Up(512, 128)\n","            self.sa4 = SelfAttention(128, img_size//4)\n","            self.up2 = Up(256, 64)\n","            self.sa5 = SelfAttention(64, img_size//2)\n","            self.up3 = Up(128, 64)\n","            self.sa6 = SelfAttention(64, img_size)\n","            self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n","            \n","        if num_classes is not None:\n","            self.label_emb = nn.Embedding(num_classes, time_dim)\n","\n","    def pos_encoding(self, t, channels, device='cuda'):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq).to(device)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq).to(device)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1).to(device)\n","        return pos_enc\n","\n","    def forward(self, x, t, y): #label\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.time_dim)\n","        \n","        if y is not None:\n","            t += self.label_emb(y)\n","\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","        x4 = self.down3(x3, t)\n","        x4 = self.sa3(x4)\n","        #print(x4.shape) torch.Size([5, 256, 8, 8])\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","        #print(x4.shape) torch.Size([5, 256, 8, 8])\n","        x = self.up1(x4, x3, t)\n","        x = self.sa4(x)\n","        x = self.up2(x, x2, t)\n","        x = self.sa5(x)\n","        x = self.up3(x, x1, t)\n","        x = self.sa6(x)\n","        output = self.outc(x)\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"e9ec378b","metadata":{"id":"e9ec378b","outputId":"1d7dda8f-fad3-4c0f-eb79-efba0d401408"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num params:  6563907\n"]}],"source":["model = UNet()\n","print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n","#model"]},{"cell_type":"code","execution_count":null,"id":"fceb8fec","metadata":{"scrolled":true,"id":"fceb8fec","outputId":"158ea9a2-cd10-463a-9cf5-a2e55dcfe5ba"},"outputs":[{"data":{"text/plain":["torch.Size([5, 256])"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"source":["def pos_encoding(t, channels):\n","    inv_freq = 1.0 / (\n","        10000\n","        ** (torch.arange(0, channels, 2).float() / channels)\n","    )\n","    pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","    pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","    pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","    return pos_enc\n","\n","t = torch.tensor([3,3,3,3,3])\n","t = t.unsqueeze(-1).type(torch.float)\n","t = pos_encoding(t, 256)\n","t.shape"]},{"cell_type":"code","execution_count":null,"id":"262b725c","metadata":{"id":"262b725c"},"outputs":[],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n","torch.cuda.empty_cache()\n","x = torch.randn([5,3,32,32])\n","#t = torch.tensor([3,3,3,3,3])\n","x1 = Down(3,128)(x,t)\n","x1 = SelfAttention(128, 16)(x1)\n","x3 = Down(128, 256)(x2, t)\n","x3 = SelfAttention(256, 8)(x3)\n","x4 = Down(256, 256)(x3, t)\n","x4 = SelfAttention(256, 4)(x4)"]},{"cell_type":"code","execution_count":null,"id":"25855db3","metadata":{"id":"25855db3","outputId":"e5b21bb2-6ef1-4cce-891b-c5894b06e2d0"},"outputs":[{"data":{"text/plain":["torch.Size([5, 256, 4, 4])"]},"execution_count":164,"metadata":{},"output_type":"execute_result"}],"source":["x4.shape"]},{"cell_type":"code","execution_count":null,"id":"5104e5af","metadata":{"id":"5104e5af","outputId":"d87cefa6-cc72-4bca-b22e-f4719c90e58b"},"outputs":[{"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Input \u001b[1;32mIn [190]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m])\n\u001b[0;32m      5\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m x_out\u001b[38;5;241m.\u001b[39mshape\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Input \u001b[1;32mIn [189]\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     61\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding(t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_dim)\n\u001b[0;32m     63\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minc(x)\n\u001b[1;32m---> 64\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa1(x2)\n\u001b[0;32m     66\u001b[0m x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x2, t)\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Input \u001b[1;32mIn [122]\u001b[0m, in \u001b[0;36mDown.forward\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool_conv(x)\n\u001b[1;32m---> 20\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m emb\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[1;32mD:\\softwares\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"]}],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n","torch.cuda.empty_cache()\n","x = torch.randn([5,3,32,32])\n","t = torch.tensor([3,3,3,3,3])\n","x_out = UNet()(x,t)\n","x_out.shape"]},{"cell_type":"code","execution_count":null,"id":"9e371360","metadata":{"id":"9e371360"},"outputs":[],"source":["print(torch.cuda.memory_summary())"]},{"cell_type":"code","execution_count":null,"id":"bf767872","metadata":{"id":"bf767872"},"outputs":[],"source":["!nvidia-smi "]},{"cell_type":"code","execution_count":null,"id":"bbc5d5fc","metadata":{"id":"bbc5d5fc"},"outputs":[],"source":["torch.cuda.empty_cache()"]}],"metadata":{"kernelspec":{"display_name":"Pytorch","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}